{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0dd79-9837-4643-b2f4-c4dc79673a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain langchain-google-genai langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc56e160-814e-4338-9b08-42f03d299690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Google Gemini API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "import os, getpass\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google Gemini API Key: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee3ce87f-197d-482d-8e63-075f30bd9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from typing import TypedDict, List\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.messages import trim_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d4cd000-ba0f-41e7-b33c-3d36e900c4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: List\n",
    "    language: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21170fa2-a5fe-4fe8-95c3-e07967c7d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "760baeb0-c24c-4434-89db-b053accab8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert career counsellor. \n",
    "Provide career guidance, skill recommendations, growth strategies, \n",
    "and suggest popular online courses (Coursera, Udemy, edX, LinkedIn Learning).\n",
    "Always answer in a supportive and structured way.\"\"\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07046b32-ecd3-4547-ad4c-77e7e6df54a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only last 5 messages OR 1000 tokens (whichever comes first)\n",
    "trimmer = RunnableLambda(\n",
    "    lambda msgs: trim_messages(\n",
    "        msgs,\n",
    "        max_tokens=1000,\n",
    "        strategy=\"last\",   # keep most recent\n",
    "        token_counter=len, # simplistic, better to use model tokenizer\n",
    "        start_on=\"human\",  # trim from first human message\n",
    "        end_on=None\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d9309f3-695c-43a1-9b2d-39f6f72aa399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: State):\n",
    "    # Trim history\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "\n",
    "    # Debug log\n",
    "    # print(f\"Messages before trim: {len(state['messages'])}, after trim: {len(trimmed_messages)}\")\n",
    "\n",
    "    # Create prompt\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "\n",
    "    # Get response\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    return {\"messages\": trimmed_messages + [response]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f74cc1d7-ad64-4a37-9567-d757f5ba401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_edge(\"model\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281e7a4-09ea-4c86-95ae-aedc409fed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"career123\"}}\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Bot: Goodbye, and best of luck in your career journey!\")\n",
    "        break\n",
    "\n",
    "    input_messages = [HumanMessage(user_input)]\n",
    "\n",
    "    full_response = []\n",
    "    for chunk, metadata in app.stream(\n",
    "        {\"messages\": input_messages, \"language\": \"English\"},\n",
    "        config,\n",
    "        stream_mode=\"messages\",\n",
    "    ):\n",
    "        if isinstance(chunk, AIMessage):\n",
    "            full_response.append(chunk.content)\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613bd01-6d70-49b8-82d6-d49c9f14fe3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# career_bot.py\n",
    "import os\n",
    "import getpass\n",
    "import json\n",
    "from typing import TypedDict, List, Optional\n",
    "\n",
    "# ------------- API Key -------------\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\n",
    "        \"Enter your Google Gemini API Key (GOOGLE_API_KEY): \"\n",
    "    )\n",
    "\n",
    "# ------------- Imports -------------\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ------------- Token counter (try tiktoken, fallback to words) -------------\n",
    "try:\n",
    "    import tiktoken\n",
    "\n",
    "    def token_count_text(text: str) -> int:\n",
    "        # cl100k_base works reasonably for many models; adjust if you have a Gemini tokenizer\n",
    "        enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(enc.encode(text))\n",
    "except Exception:\n",
    "    def token_count_text(text: str) -> int:\n",
    "        return max(1, len(text.split()))\n",
    "\n",
    "# ------------- Config (tune these) -------------\n",
    "MAX_TOKENS_HISTORY = 900     # approx tokens to keep in history\n",
    "MAX_MESSAGES_HISTORY = 8     # fallback limit to keep if token counting not perfect\n",
    "SUMMARY_ROUND_TOKEN_LIMIT = 300  # how long summary we allow (in tokens)\n",
    "\n",
    "# ------------- State definition -------------\n",
    "class State(TypedDict):\n",
    "    messages: List  # list of HumanMessage/AIMessage\n",
    "    language: str\n",
    "    summary: Optional[str]  # condensed summary of older conversation\n",
    "\n",
    "# ------------- Initialize Gemini model -------------\n",
    "# Choose your model string: \"gemini-1.5-flash\" or \"gemini-1.5-pro\" etc.\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# ------------- Prompt templates -------------\n",
    "# Main career counselor template — it uses {summary} (short user profile) and messages placeholder\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are an expert career counsellor. Use the user summary (if present) to \"\n",
    "                \"personalize answers. Provide structured, actionable advice: career suggestions, \"\n",
    "                \"skills to learn, growth strategies, and 2-4 recommended online courses (platform + course name). \"\n",
    "                \"Be concise and give bullets and next steps.\"\n",
    "            ),\n",
    "        ),\n",
    "        # optional short user-profile summary\n",
    "        (\"system\", \"User summary (if any): {summary}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Summarizer template (to compress removed old chats into a short profile)\n",
    "summarizer_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            (\n",
    "                \"You are a summarization assistant. Read the conversation below and produce \"\n",
    "                \"a concise, factual bulleted summary (3-6 bullets) containing: user's background, \"\n",
    "                \"career preferences, preferred technologies, constraints (time/location), goals, \"\n",
    "                \"and important follow-ups. Output only the bullet list (no commentary).\"\n",
    "            ),\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ------------- Helper trimming function -------------\n",
    "def simple_trim(messages, max_tokens=MAX_TOKENS_HISTORY, max_messages=MAX_MESSAGES_HISTORY):\n",
    "    \"\"\"\n",
    "    Keeps the most recent messages while staying under max_tokens or max_messages.\n",
    "    Returns (kept_messages, removed_messages).\n",
    "    \"\"\"\n",
    "    if not messages:\n",
    "        return [], []\n",
    "\n",
    "    rev = list(reversed(messages))\n",
    "    kept = []\n",
    "    token_sum = 0\n",
    "    for m in rev:\n",
    "        text = (m.content or \"\") if hasattr(m, \"content\") else str(m)\n",
    "        t = token_count_text(text)\n",
    "        if (len(kept) < max_messages) and (token_sum + t <= max_tokens):\n",
    "            kept.append(m)\n",
    "            token_sum += t\n",
    "        else:\n",
    "            # stop adding, these and everything older is removed\n",
    "            break\n",
    "    kept.reverse()\n",
    "    removed = messages[: len(messages) - len(kept)]\n",
    "    return kept, removed\n",
    "\n",
    "# ------------- Summarizer function -------------\n",
    "def summarize_removed(removed_messages, existing_summary: Optional[str] = None) -> str:\n",
    "    \"\"\"\n",
    "    Summarize removed messages using the model and append to existing_summary.\n",
    "    Keeps summaries short.\n",
    "    \"\"\"\n",
    "    if not removed_messages:\n",
    "        return existing_summary or \"\"\n",
    "\n",
    "    # Create a prompt using the summarizer_template\n",
    "    prompt = summarizer_template.invoke({\"messages\": removed_messages})\n",
    "    resp = model.invoke(prompt)\n",
    "\n",
    "    new_summary_piece = resp.content.strip()\n",
    "    if existing_summary:\n",
    "        combined = existing_summary.strip() + \"\\n\" + new_summary_piece\n",
    "    else:\n",
    "        combined = new_summary_piece\n",
    "\n",
    "    # Optionally compress the combined summary if it's long\n",
    "    # (We do a naive trim by token count: ask the model to compress)\n",
    "    if token_count_text(combined) > SUMMARY_ROUND_TOKEN_LIMIT:\n",
    "        compress_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"Compress the following summary to 4 bullet points, preserving key facts and constraints. Output only bullets.\"\n",
    "                ),\n",
    "                MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            ]\n",
    "        )\n",
    "        # wrap combined into a single HumanMessage so template accepts it\n",
    "        compressed_prompt = compress_prompt.invoke({\"messages\": [HumanMessage(combined)]})\n",
    "        compressed_resp = model.invoke(compressed_prompt)\n",
    "        combined = compressed_resp.content.strip()\n",
    "\n",
    "    return combined\n",
    "\n",
    "# ------------- The single graph node that trims, summarizes, and calls the model -------------\n",
    "def call_model(state: State):\n",
    "    # state arrives with previous memory merged by MemorySaver plus incoming messages\n",
    "    messages = state.get(\"messages\", []) or []\n",
    "    language = state.get(\"language\", \"English\")\n",
    "    existing_summary = state.get(\"summary\", \"\") or \"\"\n",
    "\n",
    "    # 1) Trim\n",
    "    kept_messages, removed_messages = simple_trim(messages)\n",
    "    # debugging prints (optional)\n",
    "    # print(f\"[debug] before={len(messages)} kept={len(kept_messages)} removed={len(removed_messages)}\")\n",
    "\n",
    "    # 2) If some messages were removed, summarize them and update summary\n",
    "    if removed_messages:\n",
    "        new_summary = summarize_removed(removed_messages, existing_summary)\n",
    "    else:\n",
    "        new_summary = existing_summary\n",
    "\n",
    "    # 3) Build final prompt and call the model.\n",
    "    #    prompt_template expects {summary} and messages placeholder\n",
    "    prompt = prompt_template.invoke({\"messages\": kept_messages, \"summary\": new_summary, \"language\": language})\n",
    "    response = model.invoke(prompt)\n",
    "\n",
    "    # 4) Return updated state: keep the trimmed history + the new response, and store updated summary\n",
    "    new_messages = kept_messages + [response]\n",
    "    return {\"messages\": new_messages, \"language\": language, \"summary\": new_summary}\n",
    "\n",
    "# ------------- Build workflow and memory -------------\n",
    "workflow = StateGraph(state_schema=State)\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_edge(\"model\", END)\n",
    "\n",
    "memory = MemorySaver()  # uses persistent storage internally (depending on langgraph version)\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# ------------- Simple CLI loop with streaming -------------\n",
    "def chat_loop():\n",
    "    thread_id = \"career_thread_001\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    print(\"CareerBot — type 'exit' to quit.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if not user_input:\n",
    "            continue\n",
    "        if user_input.lower() in (\"exit\", \"quit\"):\n",
    "            print(\"Bot: Goodbye — good luck on your career journey!\")\n",
    "            break\n",
    "\n",
    "        input_messages = [HumanMessage(user_input)]\n",
    "        # stream response pieces\n",
    "        full = []\n",
    "        for chunk, metadata in app.stream(\n",
    "            {\"messages\": input_messages, \"language\": \"English\"},\n",
    "            config,\n",
    "            stream_mode=\"messages\",\n",
    "        ):\n",
    "            if isinstance(chunk, AIMessage):\n",
    "                # print streaming chunk\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "                full.append(chunk.content)\n",
    "        print(\"\\n\")  # newline after message completes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat_loop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8039669-9060-475b-b247-fe5844e66532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
